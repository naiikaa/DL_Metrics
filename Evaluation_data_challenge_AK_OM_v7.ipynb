{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #Please change paths to the according pointing on your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fY3Ac9Wl4O1x"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import warnings\n",
    "import tables\n",
    "import calendar\n",
    "\n",
    "import torch\n",
    "import scipy\n",
    "from scipy.spatial.distance import pdist, cdist\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "import umap.umap_ as umap\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = '20231103_OM_privat_short.csv'\n",
    "file_name = '20240119_OM_privat.csv'\n",
    "\n",
    "folder_path = 'D:\\\\GWDG\\\\LSM_Challenge\\\\openmeter\\\\data\\\\202401_OM_data_for_training'\n",
    "\n",
    "file_path_os = os.path.join(folder_path, file_name)\n",
    "\n",
    "om_df = pd.read_csv(file_path_os)\n",
    "#om_df.rename(columns={'time': 'Datetime'}, inplace=True)\n",
    "#om_df['Datetime'] = pd.to_datetime(om_df['Datetime'])\n",
    "#om_df.set_index('time', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prepartion and sample generation\n",
    "\n",
    "- with columns time,MAC_IDs.... und datetime64,kwh_hh values into a tensor for the analysis in this notebook\n",
    "\n",
    "- Anka & Phil modified the original code from Stephan that was working with the LSM-data and we tried to get it work with the OM-data as well.\n",
    "    - maybe something goes wrong here that causes problems later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = \"20231306_OM.h5\"\n",
    "\n",
    "# Define your specific parameters\n",
    "min_test_required = 0.5  # Adjust this value as needed\n",
    "train_start_date = \"2022-01-01\"  # Adjust the training start date as needed\n",
    "train_end_date = \"2023-07-30\"    # Adjust the training end date as needed\n",
    "test_start_date = \"2021-01-01\"   # Adjust the testing start date as needed\n",
    "test_end_date = \"2021-12-31\"     # Adjust the testing end date as needed\n",
    "\n",
    "def filter_households(data, min_test_required=0.1):\n",
    "    n_samples_test = [len(household['power_test']) for household in data]\n",
    "    max_num_time_steps = max(n_samples_test)\n",
    "    min_abs_required = max_num_time_steps * min_test_required\n",
    "    data_filtered = [household for household, n_samples in zip(data, n_samples_test) if n_samples > min_abs_required]\n",
    "    return data_filtered\n",
    "\n",
    "def load_households_from_hdf(hdf_file, min_test_required, train_start_date, train_end_date, test_start_date, test_end_date):\n",
    "    data = []\n",
    "    with pd.HDFStore(hdf_file, mode='r') as hdf:\n",
    "        keys = hdf.keys()\n",
    "\n",
    "        for key_id, key in enumerate(keys):\n",
    "            df = hdf.get(key)\n",
    "            df['KWH_per_half_hour'] = pd.to_numeric(df['KWH_per_half_hour'], errors='coerce')\n",
    "            df.dropna(subset=['KWH_per_half_hour'], inplace=True)\n",
    "\n",
    "            numeric_columns = ['KWH_per_half_hour']\n",
    "            df = df[numeric_columns]\n",
    "\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            df = df.resample('15min').mean()\n",
    "            df.dropna(inplace=True)\n",
    "\n",
    "            power_values = df['KWH_per_half_hour'].values.astype(np.float32)\n",
    "            time = df.index.values\n",
    "\n",
    "            # Split data into training and testing based on custom time frames\n",
    "            train_bool = (time >= np.datetime64(train_start_date)) & (time <= np.datetime64(train_end_date))\n",
    "            test_bool = (time >= np.datetime64(test_start_date)) & (time <= np.datetime64(test_end_date))\n",
    "\n",
    "            train_idx = np.where(train_bool)[0]\n",
    "            test_idx = np.where(test_bool)[0]\n",
    "\n",
    "            # Split power values into training and testing\n",
    "            power_train = power_values[train_idx]\n",
    "            power_test = power_values[test_idx]\n",
    "\n",
    "            time_train = time[train_idx]\n",
    "            time_test = time[test_idx]\n",
    "\n",
    "            data.append(dict(\n",
    "                key=key,\n",
    "                power_train=power_train,\n",
    "                power_test=power_test,\n",
    "                time_train=time_train,\n",
    "                time_test=time_test\n",
    "            ))\n",
    "\n",
    "        hdf.close()\n",
    "\n",
    "    data = filter_households(data, min_test_required)\n",
    "    return data\n",
    "\n",
    "file_out = \"20231306_OM.h5\"  # Replace with the actual file path\n",
    "data = load_households_from_hdf(file_out, min_test_required, train_start_date, train_end_date, test_start_date, test_end_date)\n",
    "print(\"\\nNumber of households:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OM_original data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '20240119_OM_privat.csv'\n",
    "folder_path = 'D:\\\\GWDG\\\\LSM_Challenge\\\\openmeter\\\\data\\\\202401_OM_data_for_training'\n",
    "file_path_os = os.path.join(folder_path, file_name)\n",
    "\n",
    "om_df = pd.read_csv(file_path_os)\n",
    "om_df.rename(columns={'time': 'Datetime'}, inplace=True)\n",
    "om_df['Datetime'] = pd.to_datetime(om_df['Datetime'])\n",
    "#om_df = om_df.drop(idtokick, axis=1)\n",
    "#om_df.set_index('time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om_df['Datetime'] = om_df['Datetime'].astype('datetime64[ns]')\n",
    "\n",
    "om_samples = dict()\n",
    "\n",
    "for col in om_df:\n",
    "  if col != 'Datetime':\n",
    "    key = f'/{col}'\n",
    "    om_samples[key] = dict()\n",
    "    om_samples[key]['samples'] = om_df[col].values\n",
    "    om_samples[key]['times'] = om_df['Datetime'].values\n",
    "    \n",
    "#om_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGAN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name2 = 'BenBasicEmbeddingCManyToManyHybridLSTMFFWGANprivateScaled.csv'\n",
    "folder_path2 = 'D:\\\\GWDG\\LSM_Challenge\\\\datachallenge_NOV23\\\\WGAN'\n",
    "file_path_os2 = os.path.join(folder_path2, file_name2)\n",
    "\n",
    "WGAN_df = pd.read_csv(file_path_os2)\n",
    "WGAN_df['Datetime'] = WGAN_df['Datetime'].astype('datetime64[ns]')\n",
    "\n",
    "#WGAN_df = WGAN_df.drop(idtokick, axis=1)   # drop selected households from analysis\n",
    "#WGAN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "WGAN_df['Datetime'] = WGAN_df['Datetime'].astype('datetime64[ns]')\n",
    "\n",
    "WGAN_samples = dict()\n",
    "\n",
    "for col in WGAN_df:\n",
    "  if col != 'Datetime':\n",
    "    key = f'/{col}'\n",
    "    WGAN_samples[key] = dict()\n",
    "    WGAN_samples[key]['samples'] = WGAN_df[col].values\n",
    "    #ddpm_samples[key]['times'] = ddpm_df['Datetime'].values.astype('datetime64[ns]')\n",
    "    WGAN_samples[key]['times'] = WGAN_df['Datetime'].values\n",
    "\n",
    "#WGAN_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name3 = 'ddpm_privat_short.csv'\n",
    "folder_path3 = 'D:\\\\GWDG\\LSM_Challenge\\\\datachallenge_NOV23\\\\DDPM'\n",
    "file_path_os3 = os.path.join(folder_path3, file_name3)\n",
    "\n",
    "ddpm_df = pd.read_csv(file_path_os3)\n",
    "ddpm_df.columns = ddpm_df.columns.str.replace('/', '')\n",
    "ddpm_df['Datetime'] = ddpm_df['Datetime'].astype('datetime64[ns]')\n",
    "\n",
    "#\n",
    "#ddpm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ddpm_samples = dict()\n",
    "\n",
    "for col in ddpm_df:\n",
    "  if col != 'Datetime':\n",
    "    key = f'/{col}'\n",
    "    ddpm_samples[key] = dict()\n",
    "    ddpm_samples[key]['samples'] = ddpm_df[col].values\n",
    "    #ddpm_samples[key]['times'] = ddpm_df['Datetime'].values.astype('datetime64[ns]')\n",
    "    ddpm_samples[key]['times'] = ddpm_df['Datetime'].values\n",
    "\n",
    "#ddpm_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Generative Models\n",
    "- currently training not working. \n",
    "    - *I couldn't really figure out why, but don't see a big advantage to train these models anyway.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTemplate(object):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  \n",
    "  def train(self, data):\n",
    "    pass\n",
    "  \n",
    "  def sample(self, data, n_samples):\n",
    "    pass\n",
    "\n",
    "class MultivariateNormal(ModelTemplate):\n",
    "  def __init__(self):\n",
    "    self.mdl_dict = dict()\n",
    "  \n",
    "  def train(self, data):\n",
    "    self.mdl_dict\n",
    "    for household in tqdm(data):\n",
    "      key = household['key']\n",
    "      time_train = household['time_train']\n",
    "      power_train = household['power_train']\n",
    "      X_train = timeseries_to_day_matrix(time_train, power_train)\n",
    "      mu = np.mean(X_train, axis=0)\n",
    "      covMat = np.cov(X_train.T)\n",
    "      self.mdl_dict[key] = dict(mu=mu, covMat=covMat)\n",
    "  \n",
    "  def sample(self, data):\n",
    "    results = dict()\n",
    "    for household in tqdm(data):\n",
    "      key = household['key']\n",
    "      time_test = household['time_test']\n",
    "      power_test = household['power_test']\n",
    "      _, time_mat = timeseries_to_day_matrix(time_test, power_test, return_time_mat=True)\n",
    "      mu = self.mdl_dict[key]['mu']\n",
    "      covMat = self.mdl_dict[key]['covMat']\n",
    "      results[key] = dict()\n",
    "      results[key]['samples'] = np.random.multivariate_normal(mean=mu, cov=covMat, size=(time_mat.shape[0],)).reshape(-1)\n",
    "      results[key]['times'] = time_mat.reshape(-1)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixtureModel(ModelTemplate):\n",
    "  def __init__(self):\n",
    "    self.mdl_dict = dict()\n",
    "  \n",
    "  def train(self, data):\n",
    "    self.mdl_dict\n",
    "    for household in tqdm(data):\n",
    "      key = household['key']\n",
    "      time_train = household['time_train']\n",
    "      power_train = household['power_train']\n",
    "      X_train = timeseries_to_day_matrix(time_train, power_train)\n",
    "      mdl = GaussianMixture(n_components=5)\n",
    "      mdl.fit(X_train)\n",
    "      self.mdl_dict[key] = mdl\n",
    "\n",
    "  def sample(self, data):\n",
    "    results = dict()\n",
    "    for household in tqdm(data):\n",
    "      key = household['key']\n",
    "      time_test = household['time_test']\n",
    "      power_test = household['power_test']\n",
    "      _, time_mat = timeseries_to_day_matrix(time_test, power_test, return_time_mat=True)\n",
    "      mdl = self.mdl_dict[key]\n",
    "      X_fake = mdl.sample(n_samples=time_mat.shape[0])[0]\n",
    "      results[key] = dict()\n",
    "      results[key]['samples'] = X_fake.reshape(-1)\n",
    "      results[key]['times'] = time_mat.reshape(-1)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = GaussianMixtureModel()\n",
    "mdl.train(data)\n",
    "gmm_samples = mdl.sample(data)\n",
    "\n",
    "plt.plot(gmm_samples['/001dce72-b1c1-44b1-bf16-0c22cfe3a420']['times'][:700].T, gmm_samples['/001dce72-b1c1-44b1-bf16-0c22cfe3a420']['samples'][:700].T)\n",
    "print(len(gmm_samples['/001dce72-b1c1-44b1-bf16-0c22cfe3a420']['samples']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = MultivariateNormal()\n",
    "mdl.train(data)\n",
    "mvn_samples = mdl.sample(data)\n",
    "\n",
    "plt.plot(mvn_samples['/001dce72-b1c1-44b1-bf16-0c22cfe3a420']['times'][:700].T, mvn_samples['/001dce72-b1c1-44b1-bf16-0c22cfe3a420']['samples'][:700].T)\n",
    "print(len(mvn_samples['/001dce72-b1c1-44b1-bf16-0c22cfe3a420']['samples']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vxvWe-pwiwEL"
   },
   "source": [
    "# Define Metrics & Evaluation Tools\n",
    "\n",
    "- still the using the older version (the a little below) of the MMD (April 2023) as a newer implementation (latest Ben, below) produced errors for me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MMD score latest; by Ben\n",
    "# def mmd(x_real: torch.Tensor, x_fake: torch.Tensor):\n",
    "#     \"\"\"\n",
    "#     Calculates the Maximum-Mean-Discrepancy between two tensors, using the (optimal) bandwidth calculated below;\n",
    "#     and a Gaussian kernel.\n",
    "#     Args:\n",
    "#         x_real: tensor of (real) data of shape [n_samples, n_features, seq_len]\n",
    "#         x_fake: tensor of (dake) data of shape [n_samples, n_features, seq_len]\n",
    "#     \"\"\"\n",
    "#     n_1, f_1, seq_1 = x_real.shape\n",
    "#     n_2, f_2, seq_2 = x_fake.shape\n",
    "#     x_real = x_real.reshape(n_1, f_1 * seq_1).cpu()\n",
    "#     x_fake = x_fake.reshape(n_2, f_2 * seq_2).cpu()\n",
    "\n",
    "#     sigma = optim_bw_mmd(x_real, x_fake)\n",
    "\n",
    "#     xy = torch.cat([x_real, x_fake], dim=0)  # concatenate: new size [n_1+n_2,-1]\n",
    "#     distances = torch.cdist(xy, xy, p=2)  # pairwise distances in L^2\n",
    "#     k = torch.exp(\n",
    "#         -(distances ** 2) / (2 * sigma ** 2))  # + epsilon * torch.eye(n_1 + n_2)  # 2. for numerical stability\n",
    "#     k_x = k[:n_1, :n_1]\n",
    "#     k_y = k[n_1:, n_1:]\n",
    "#     k_xy = k[:n_1, n_1:]\n",
    "\n",
    "#     mmd_score = k_x.sum() / (n_1 * (n_1 - 1)) + k_y.sum() / (n_2 * (n_2 - 1)) - 2 * k_xy.sum() / (n_1 * n_2)\n",
    "\n",
    "#     return mmd_score\n",
    "\n",
    "\n",
    "# def optim_bw_mmd(x_real, x_fake):\n",
    "#     \"\"\"\n",
    "#     Finds the optimal bandwidth for calculating the MMD score.\n",
    "#     \"\"\"\n",
    "#     n_1 = len(x_real)\n",
    "#     n_2 = len(x_fake)\n",
    "#     x_real = x_real.view(n_1, -1)\n",
    "#     x_fake = x_fake.view(n_2, -1)\n",
    "\n",
    "#     distances = torch.pdist(torch.cat([x_real, x_fake], dim=0))\n",
    "#     indx = np.random.permutation(len(distances))[:500]\n",
    "#     sigma = distances[indx].median() / 2\n",
    "\n",
    "#     return sigma\n",
    "\n",
    "# def rbf_kernel_dist(dist, sigma):\n",
    "#     return torch.exp(- torch.square(dist) / (2*torch.square(sigma)))\n",
    "\n",
    "# def new_mmd(x:torch.Tensor, y:torch.Tensor, device:str):\n",
    "#     \"\"\"\n",
    "#     Calculates the Maximum-Mean-Discrepancy between two tensors, using the (optimal) bandwidth calculated below;\n",
    "#     and a Gaussian kernel.\n",
    "#     Implemented as in 'A kernel method for the two-sample problem', Gretton et. al. (2012)\n",
    "#     Args:\n",
    "#         X: tensor of (real) data of shape [n1_samples, seq_len]\n",
    "#         Y: tensor of (fake) data of shape [n2_samples, seq_len]\n",
    "#         device: string of the device i.e. 'cpu' for cpu, 'cuda' for gpu, 'cuda:1' for gpu 2 etc.\n",
    "#     \"\"\"\n",
    "#     x = x.to(device)\n",
    "#     y = y.to(device)\n",
    "#     x_dist = torch.pdist(x).flatten()\n",
    "#     y_dist = torch.pdist(y).flatten()\n",
    "#     xy_dist = torch.cdist(x,y).flatten()\n",
    "#     full_dists = torch.cat([x_dist, y_dist, xy_dist]).cpu()\n",
    "#     sigma = full_dists.median().unsqueeze(0).to(device)\n",
    "#     m = x.shape[0]\n",
    "#     n = y.shape[0]\n",
    "\n",
    "#     x_kernels = rbf_kernel_dist(x_dist, sigma)\n",
    "#     y_kernels = rbf_kernel_dist(y_dist, sigma)\n",
    "#     xy_kernels = rbf_kernel_dist(xy_dist, sigma)\n",
    "\n",
    "#     return (torch.sum(x_kernels)*2/(m*(m-1)) -  2 * torch.sum(xy_kernels) / (m*n) + torch.sum(y_kernels)*2/(n*(n-1))).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_to_day_matrix(time, power, return_time_mat=False, drop_na=True):\n",
    "  \"\"\"\n",
    "  Reshapes time series into a matrix, where each column corresponds to \n",
    "  time of day and each row corresponds to a day\n",
    "  X_train = timeseries_to_day_matrix(time_train, power_train)\n",
    "  X_test = timeseries_to_day_matrix(time_test, power_test)\n",
    "  \"\"\"\n",
    "  df = pd.DataFrame(dict(time=time, power=power))\n",
    "  df.set_index('time', inplace=True)\n",
    "  df = df.resample('30min').mean()\n",
    "  min_date = np.min(df.index)\n",
    "  bool_idx = df.index >= (np.datetime64(f'{min_date.year}-{min_date.month:02.0f}-{min_date.day:02.0f}') + 1)\n",
    "  max_date = np.max(df.index)\n",
    "  bool_idx = np.logical_and(bool_idx, \n",
    "    (df.index < np.datetime64(f'{max_date.year}-{max_date.month:02.0f}-{max_date.day:02.0f}') - 1))\n",
    "  df = df[bool_idx]\n",
    "  power_mat = df['power'].values.reshape(-1,48)\n",
    "  time_mat = df.index.values.reshape(-1,48)\n",
    "  if drop_na:\n",
    "    keep_idx = np.logical_not(np.any(np.isnan(power_mat), axis=1))\n",
    "    power_mat = power_mat[keep_idx]\n",
    "    time_mat = time_mat[keep_idx]\n",
    "\n",
    "  if return_time_mat:\n",
    "    return power_mat, time_mat\n",
    "  else:\n",
    "    return power_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "omeHjldz2o6O",
    "outputId": "f0de19ba-5afb-45f6-eea6-aef229205763"
   },
   "outputs": [],
   "source": [
    "# Maximum-Mean-Discrepancy\n",
    "def mmd(x_real, x_fake):\n",
    "    \"\"\"\n",
    "    Calculates the Maximum-Mean-Discrepancy between two tensors, using the (optimal) bandwidth calculated below;\n",
    "    and a Gaussian kernel.\n",
    "    Args:\n",
    "        x_real: tensor of (real) data of shape [n_samples, n_features, seq_len]\n",
    "        x_fake: tensor of (fake) data of shape [n_samples, n_features, seq_len]\n",
    "    \"\"\"\n",
    "\n",
    "    if (len(x_real.shape)==3):\n",
    "      n_1, f_1, seq_1 = x_real.shape\n",
    "      n_2, f_2, seq_2 = x_fake.shape\n",
    "      x_real = x_real.reshape(n_1, f_1 * seq_1)\n",
    "      x_fake = x_fake.reshape(n_2, f_2 * seq_2)\n",
    "    elif (len(x_real.shape)==2):\n",
    "      n_1, seq_1 = x_real.shape\n",
    "      n_2, seq_2 = x_fake.shape\n",
    "    else:\n",
    "      raise ValueError(\"Only 2 or 3 dimensional tensors are possible.\")\n",
    "\n",
    "    sigma = optim_bw_mmd(x_real, x_fake)\n",
    "\n",
    "    xy = np.concatenate([x_real, x_fake], axis=0)  # concatenate: new size [n_1+n_2,-1]\n",
    "    #print(xy.shape)\n",
    "    distances = cdist(xy, xy); #torch.cdist(xy, xy, p=2)  # pairwise distances in L^2\n",
    "    k = np.exp(\n",
    "        -(distances ** 2) / (2 * sigma ** 2))  # + epsilon * torch.eye(n_1 + n_2)  # 2. for numerical stability\n",
    "    k_x = k[:n_1, :n_1]\n",
    "    k_y = k[n_1:, n_1:]\n",
    "    k_xy = k[:n_1, n_1:]\n",
    "\n",
    "    mmd_score = k_x.sum() / (n_1 * (n_1 - 1)) + k_y.sum() / (n_2 * (n_2 - 1)) - 2 * k_xy.sum() / (n_1 * n_2)\n",
    "\n",
    "    return mmd_score\n",
    "\n",
    "def optim_bw_mmd(x_real, x_fake):\n",
    "    \"\"\"\n",
    "    Finds the optimal bandwidth for calculating the MMD score.\n",
    "    \"\"\"\n",
    "    n_1 = len(x_real)\n",
    "    n_2 = len(x_fake)\n",
    "    x_real = x_real.reshape(n_1, -1)\n",
    "    x_fake = x_fake.reshape(n_2, -1)\n",
    "\n",
    "    distances = pdist(np.concatenate([x_real, x_fake], axis=0))\n",
    "    #indx = np.random.permutation(len(distances))[:500]\n",
    "    indx = np.arange(len(distances))\n",
    "    sigma = np.median(distances[indx]) / 2\n",
    "\n",
    "    return sigma\n",
    "\n",
    "\n",
    "def compute_household_mmd(data, **kwargs):\n",
    "  results = {mdl_name: [] for mdl_name in kwargs.keys()}\n",
    "  for household in tqdm(data):\n",
    "    time_test, power_test = household['time_test'], household['power_test']\n",
    "    key = household['key']\n",
    "    X_test = timeseries_to_day_matrix(time_test, power_test)\n",
    "    for mdl_name, samples in kwargs.items():\n",
    "      time_fake = samples[key]['times']\n",
    "      power_fake = samples[key]['samples']\n",
    "      X_fake = timeseries_to_day_matrix(time_fake, power_fake)\n",
    "      mmd_value = mmd(X_fake, X_test)\n",
    "      results[mdl_name].append(mmd_value)\n",
    "      \n",
    "  return results\n",
    "\n",
    "\n",
    "household_mmds = compute_household_mmd(data,\n",
    "  OM=om_samples, #OM_samples used as control input as the MMD should be 0 ?!?\n",
    "  DDPM=ddpm_samples,\n",
    "  WGAN=WGAN_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "HVCmnhaJQpxR",
    "outputId": "1baef73f-f8f1-4cca-da84-d5b9d45586ee"
   },
   "outputs": [],
   "source": [
    "def boxplot_day_mmd(results, baseline_mdl):  \n",
    "  results_df = pd.DataFrame(results)\n",
    "  plt.figure(figsize=(15,1.5*len(results.keys())))\n",
    "  results_df.boxplot(vert=False, notch=True, showmeans=True, meanprops=dict(marker='s', markersize=10, markerfacecolor='black', markeredgecolor='black'), medianprops=dict(linewidth=7, color='red'), widths=0.5, boxprops= dict(linewidth=5))\n",
    "  plt.xlabel('MMD score', fontsize=28)\n",
    "  plt.yticks(fontsize=28)\n",
    "  plt.xticks(fontsize=20)\n",
    "\n",
    "  for k, mdl_name in enumerate(results.keys()):\n",
    "    pairwise_differences = np.array(results[baseline_mdl]) - np.array(results[mdl_name])\n",
    "    if mdl_name != baseline_mdl:\n",
    "      test_results = scipy.stats.ttest_1samp(pairwise_differences.ravel(), 0, alternative='two-sided')\n",
    "      plt.text(1, k+0.6, f'p-value = {test_results.pvalue:e<0.3}', fontsize=22)\n",
    "    else:\n",
    "      plt.text(1, k+0.6, f'Baseline', fontsize=22)\n",
    "  #plt.title(f'Boxplot and p-Values of Pairwise two-sided t-Test', fontsize=16)\n",
    "\n",
    "boxplot_day_mmd(household_mmds, baseline_mdl='BASELINE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diurnal plots\n",
    "\n",
    "- some weird behavior going on here.\n",
    "\n",
    "- I tried to replicate the loading part above so that the rest of the Analysis would work like for the LSM-challenge (April 24)\n",
    "\n",
    "- the samples however, don't really work out here, as my later plor shows quit good results, the plots here are all over the place\n",
    "    - *thats why I dont't really trust the later UMAP-results really*\n",
    "        - *my guess would be, that I messed something up during the sample-generation before*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Yg70nvra8oDT",
    "outputId": "ccc765b8-8175-43ba-e582-5b4cf8fd3aa4"
   },
   "outputs": [],
   "source": [
    "def diurnal_average_plot(data, samples, mdl_name):\n",
    "  plt.figure(figsize=(20,15))\n",
    "\n",
    "  for k, household in enumerate(data):\n",
    "    if k>15:\n",
    "      break\n",
    "    time_test, power_test = household['time_test'], household['power_test']\n",
    "    key = household['key']\n",
    "\n",
    "    X_test = timeseries_to_day_matrix(time_test, power_test)\n",
    "\n",
    "    time_fake = samples[key]['times']\n",
    "    power_fake = samples[key]['samples']\n",
    "    X_fake = timeseries_to_day_matrix(time_fake, power_fake)\n",
    "\n",
    "    mean_day_test = np.mean(X_test, axis=0)\n",
    "    mean_day_mvn = np.mean(X_fake, axis=0)\n",
    "\n",
    "    prctl_test = np.percentile(X_test, q=[5, 95], axis=0)\n",
    "    prctl_mvn = np.percentile(X_fake, q=[5, 95], axis=0)\n",
    "\n",
    "    hour = np.arange(0, 24, step=0.5)\n",
    "    plt.subplot(4, 4, k+1)\n",
    "    plt.plot(hour, mean_day_test, 'g', label='Real')\n",
    "    plt.plot(hour, prctl_test.T, 'g--')\n",
    "    plt.plot(hour, mean_day_mvn, 'b', label=mdl_name)\n",
    "    plt.plot(hour, prctl_mvn.T, 'b--')\n",
    "    plt.xticks([0,6,12,18,24])\n",
    "    plt.legend()\n",
    "    plt.xlabel('hour of day')\n",
    "    plt.title(f'diurnal cycle, {key}')\n",
    "    plt.ylabel('power [kWh]')\n",
    "    plt.grid('on')\n",
    "    plt.xlim([0, 24])\n",
    "    plt.ylim([-.5, 3.5])\n",
    "  plt.subplots_adjust(hspace=0.5)\n",
    "  plt.show()\n",
    "\n",
    "# diurnal_average_plot(data, gmm_samples, mdl_name='GMM')\n",
    "# print(\"\\n\"*1)\n",
    "# diurnal_average_plot(data, mvn_samples, mdl_name='MVN')\n",
    "# print(\"\\n\"*1)\n",
    "# diurnal_average_plot(data, hmm_samples, mdl_name='HMM')\n",
    "# print(\"\\n\"*1)\n",
    "diurnal_average_plot(data, ddpm_samples, mdl_name='DDPM')\n",
    "print(\"\\n\"*1)\n",
    "# diurnal_average_plot(data, con_bnf_samples, mdl_name='con_BNF')\n",
    "# print(\"\\n\"*1)\n",
    "# diurnal_average_plot(data, con_bnf_samples2, mdl_name='con_MAF_BNF')\n",
    "# print(\"\\n\"*1)\n",
    "# diurnal_average_plot(data, CNN_VAE_samples, mdl_name='CNN_VAE')\n",
    "# print(\"\\n\"*1)\n",
    "diurnal_average_plot(data, WGAN_samples, mdl_name='WGAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I would say that the violin-plots are also \"bullshit\" due to badly prepared samples; thus cannor really interpreted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1nsTY5dj0ARj",
    "outputId": "780c4e22-9883-455a-9943-6f4449af9172"
   },
   "outputs": [],
   "source": [
    "\n",
    "def marginal_dist_violinplots(data, **model_samples):\n",
    "  plt.figure(figsize=(25,35))\n",
    "  \n",
    "  for k, household in enumerate(data):\n",
    "    if k>=16:\n",
    "      break\n",
    "    plt.subplot(6, 3, k+1)\n",
    "    key = household['key']\n",
    "    data_list = [household['power_test']]\n",
    "    label_list = ['Real']\n",
    "    for mdl_name, samples in model_samples.items():\n",
    "      data_list.append(samples[key]['samples'])\n",
    "      label_list.append(mdl_name)\n",
    "    plt.violinplot(data_list, \n",
    "                  showmeans=False, \n",
    "                  showmedians=True, \n",
    "                  showextrema=True, \n",
    "                  vert=False)\n",
    "    plt.yticks([1, 2, 3], label_list, rotation=0, fontsize=18)\n",
    "    plt.xlabel('Power [kWh/30min]', fontsize=18)\n",
    "    plt.xlim([-1.2, 4500])\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.title(f'Marginal Distribution, {key}', fontsize=20)\n",
    "    plt.grid('on')\n",
    "  plt.subplots_adjust(hspace=0.4)\n",
    "  plt.subplots_adjust(wspace=0.3)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "marginal_dist_violinplots(data, \n",
    "  # GMM=gmm_samples, \n",
    "  # MVN=mvn_samples,\n",
    "  # HMM=hmm_samples,\n",
    "  # BNF=con_bnf_samples,\n",
    "  # MAF_BNF=con_bnf_samples2,\n",
    "  DDPM=ddpm_samples,\n",
    "  # CNN_VAE=CNN_VAE_samples,\n",
    "  WGAN=WGAN_samples  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- same here. The Clustering looks way worse than what I would expect from the data-traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fthV7_U-Sefx",
    "outputId": "a7ad12fe-e72a-4e0e-be01-421a7ce9b718"
   },
   "outputs": [],
   "source": [
    "\n",
    "def umap_plots(data, samples, mdl_name, n_households=5):\n",
    "\n",
    "  X_real_list = []\n",
    "  X_fake_list = []\n",
    "\n",
    "  id_list = []\n",
    "  fake_flag = []\n",
    "  key_list = []\n",
    "  for k, household in enumerate(data):\n",
    "    if k>=n_households:\n",
    "      break\n",
    "    real_time = household['time_test']\n",
    "    real_power = household['power_test']\n",
    "    key = household['key']\n",
    "\n",
    "    key_list.append(key)\n",
    "\n",
    "    X_real = timeseries_to_day_matrix(real_time, real_power)\n",
    "    X_real_list.append(X_real)\n",
    "\n",
    "    fake_time = samples[key]['times']\n",
    "    fake_power = samples[key]['samples']\n",
    "    X_fake = timeseries_to_day_matrix(fake_time, fake_power)\n",
    "    X_fake_list.append(X_fake)\n",
    "\n",
    "    id = np.ones(X_real.shape[0] + X_fake.shape[0])*k\n",
    "    id_list.append(id)\n",
    "\n",
    "    fake = np.ones(X_real.shape[0] + X_fake.shape[0])\n",
    "    fake[:X_real.shape[0]] = 0\n",
    "    fake_flag.append(fake)\n",
    "    \n",
    "\n",
    "  id_list = np.concatenate(id_list)\n",
    "  fake_flag = np.concatenate(fake_flag)\n",
    "\n",
    "  X_real_all = np.concatenate(X_real_list, axis=0)\n",
    "  X_fake_all = np.concatenate(X_fake_list, axis=0)\n",
    "\n",
    "  X_all = np.concatenate([X_real_all, X_fake_all], axis=0);\n",
    "\n",
    "\n",
    "  print(id_list.shape)\n",
    "  print(X_all.shape)\n",
    "\n",
    "  # standardize columns\n",
    "  mean_all = np.mean(X_all, axis=0)\n",
    "  std_all = np.std(X_all, axis=0)\n",
    "  X_all = (X_all-mean_all)/std_all\n",
    "\n",
    "  # Define UMAP parameters\n",
    "  n_components = 2\n",
    "  n_neighbors = 10\n",
    "  min_dist = 1.\n",
    "\n",
    "  # Create a UMAP model and fit the data\n",
    "  umap_model = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist)\n",
    "  umap_data = umap_model.fit_transform(X_all)\n",
    "\n",
    "  # Plot the results\n",
    "  plt.figure(figsize=(20,10))\n",
    "  plt.subplot(1,2,1)\n",
    "  for k, key in enumerate(key_list):\n",
    "    color = np.random.rand(3)\n",
    "    color = (color-color.min())/(color.max()-color.min())\n",
    "    fltr = np.logical_and(id_list==k, fake_flag==0)\n",
    "    plt.plot(umap_data[fltr, 0], umap_data[fltr, 1], '.', color=color, alpha=0.9, label=f'{key}, real')\n",
    "    fltr = np.logical_and(id_list==k, fake_flag==1)\n",
    "    plt.plot(umap_data[fltr, 0], umap_data[fltr, 1], 'x', color=color, alpha=0.9, label=f'{key}, fake')\n",
    "  plt.legend()\n",
    "\n",
    "  #plt.scatter(umap_data[:, 0], umap_data[:, 1], c=id_list, s=10, alpha=0.9)\n",
    "  plt.grid('on')\n",
    "  plt.title(f\"{mdl_name}\\nUMAP projection of single days\\n colored by household nr.\")\n",
    "  plt.xlabel('Embedding Dimension #1')\n",
    "  plt.xlabel('Embedding Dimension #2')\n",
    "\n",
    "  # Plot the results\n",
    "  plt.subplot(1,2,2)\n",
    "  #plt.scatter(umap_data[:, 0], umap_data[:, 1], c=fake_flag, s=10, alpha=0.9)\n",
    "  plt.plot(umap_data[fake_flag==0, 0], umap_data[fake_flag==0, 1], '.', alpha=0.9, label='Real')\n",
    "  plt.plot(umap_data[fake_flag==1, 0], umap_data[fake_flag==1, 1], '.', alpha=0.9, label='Fake')\n",
    "  plt.grid('on')\n",
    "  plt.legend()\n",
    "  plt.title(f\"{mdl_name}\\nUMAP projection  of single days\\n colored by real or fake\")\n",
    "  plt.xlabel('Embedding Dimension #1')\n",
    "  plt.xlabel('Embedding Dimension #2')\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "# umap_plots(data, gmm_samples, mdl_name='Gaussian Mixutre Model')\n",
    "# umap_plots(data, mvn_samples, mdl_name='Multivariate Normal Model')\n",
    "# umap_plots(data, con_bnf_samples, mdl_name='BNF')\n",
    "# umap_plots(data, con_bnf_samples2, mdl_name='MAF_BNF')\n",
    "# umap_plots(data, hmm_samples, mdl_name='HMM')\n",
    "umap_plots(data, ddpm_samples, mdl_name='DDPM')\n",
    "# umap_plots(data, CNN_VAE_samples, mdl_name='CNN_VAE')\n",
    "umap_plots(data, WGAN_samples, mdl_name='WGAN')\n",
    "umap_plots(data, om_samples, mdl_name='OM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 920
    },
    "id": "5TtoKLgsfErW",
    "outputId": "537db724-e7a2-4883-9ab0-1d8c962a6c19"
   },
   "outputs": [],
   "source": [
    "n_households=5\n",
    "model_samples = dict(\n",
    "  # GMM=gmm_samples, \n",
    "  # MVN=mvn_samples,\n",
    "  # HMM=hmm_samples,\n",
    "  DDPM=ddpm_samples,\n",
    "  # MAF_BNF=con_bnf_samples2,\n",
    "  # BNF=con_bnf_samples,\n",
    "  # VAE=CNN_VAE_samples,\n",
    "  WGAN=WGAN_samples,\n",
    "  OM=om_samples\n",
    ")\n",
    "#def umap_plots(data, n_households=5, **model_samples):\n",
    "\n",
    "X_list = []\n",
    "\n",
    "\n",
    "household_id_list = []\n",
    "fake_flag = []\n",
    "key_list = []\n",
    "mdl_list = []\n",
    "mdl_names = ['Real']\n",
    "mdl_ids = [-1]\n",
    "\n",
    "for k, household in enumerate(data):\n",
    "  if k>=n_households:\n",
    "    break\n",
    "  real_time = household['time_test']\n",
    "  real_power = household['power_test']\n",
    "  key = household['key']\n",
    "\n",
    "  key_list.append(key)\n",
    "\n",
    "  X_real = timeseries_to_day_matrix(real_time, real_power)\n",
    "  X_list.append(X_real)\n",
    "\n",
    "  household_id = np.ones(X_real.shape[0])*k\n",
    "  household_id_list.append(household_id)\n",
    "  fake = np.ones(X_real.shape[0])\n",
    "  fake_flag.append(fake)\n",
    "  mdl_list.extend([-1]*X_real.shape[0])\n",
    "  for mdl_id, (mdl_name, samples) in enumerate(model_samples.items()):\n",
    "    fake_time = samples[key]['times']\n",
    "    fake_power = samples[key]['samples']\n",
    "    X_fake = timeseries_to_day_matrix(fake_time, fake_power)\n",
    "    X_list.append(X_fake)\n",
    "\n",
    "    household_id = np.ones(X_fake.shape[0])*k\n",
    "    household_id_list.append(household_id)\n",
    "    fake = np.zeros(X_fake.shape[0])\n",
    "    fake_flag.append(fake)\n",
    "    mdl_list.extend([mdl_id]*X_fake.shape[0])\n",
    "    if k==0:\n",
    "      mdl_ids.append(mdl_id)\n",
    "      mdl_names.append(mdl_name)\n",
    "\n",
    "household_id_list = np.concatenate(household_id_list)\n",
    "fake_flag = np.concatenate(fake_flag)\n",
    "mdl_list = np.array(mdl_list)\n",
    "\n",
    "X_all = np.concatenate(X_list, axis=0)\n",
    "\n",
    "print(X_all.shape)\n",
    "print(mdl_list.shape)\n",
    "print(fake_flag.shape)\n",
    "print(household_id_list.shape)\n",
    "\n",
    "# standardize columns\n",
    "mean_all = np.mean(X_all, axis=0)\n",
    "std_all = np.std(X_all, axis=0)\n",
    "X_all = (X_all-mean_all)/std_all\n",
    "\n",
    "# Define UMAP parameters\n",
    "n_components = 2\n",
    "n_neighbors = 10\n",
    "min_dist = 1.\n",
    "\n",
    "# Create a UMAP model and fit the data\n",
    "umap_model = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist)\n",
    "umap_data = umap_model.fit_transform(X_all)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "for k, (mdl_id, mdl_name) in enumerate(zip(mdl_ids, mdl_names)):\n",
    "  fltr = mdl_list==mdl_id\n",
    "  plt.plot(umap_data[fltr, 0], umap_data[fltr, 1], '.', alpha=0.5, label=f'{mdl_name}')\n",
    "plt.legend()\n",
    "\n",
    "\"\"\"\n",
    "umap_plots(data, \n",
    "  # GMM=gmm_samples, \n",
    "  # MVN=mvn_samples,\n",
    "  DDPM=ddpm_samples,\n",
    "  HMM=hmm_samples,\n",
    "  MAF_BNF=con_bnf_samples2,\n",
    "  BNF=con_bnf_samples,\n",
    "  VAE=CNN_VAE_samples,\n",
    "  WGAN=wgan_samples\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbour 24h plot of samples from UMAP\n",
    "\n",
    "- the single Traces here confirm more or less that the samples even from the original Data look kind of bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "KDgRB__7lLnd",
    "outputId": "22d12b01-1243-4769-f107-3b100d1936cf"
   },
   "outputs": [],
   "source": [
    "def plot_neighbors(umap_data, target_position, n = 10):\n",
    "  target_position = np.array(target_position).reshape(1,2)\n",
    "  idx_sort = np.argsort(np.sum(np.square(umap_data - target_position), axis=1))\n",
    "  target_samples = X_all[idx_sort[:n]]\n",
    "  target_mdls = mdl_list[idx_sort[:n]]\n",
    "\n",
    "  color_list = ['k', 'r', 'b', 'orange']\n",
    "  for ts, tmdls in zip(target_samples, target_mdls):\n",
    "    plt.plot(np.arange(0, 24, 0.5), ts, color_list[tmdls+1], label=mdl_names[tmdls+1])\n",
    "  plt.legend()\n",
    "  plt.xlabel('Time of Day [h]')\n",
    "  plt.ylabel('Power [kWh / 30min]')\n",
    "  \n",
    "\n",
    "plot_neighbors(umap_data, [9, 7], n = 7)\n",
    "plt.show()\n",
    "\n",
    "plot_neighbors(umap_data, [15, 7.5], n = 7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "J_7whdnnDnpg",
    "outputId": "e3043d94-1e55-45f2-db38-34cc75c9bcb8"
   },
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "kmeans_mdl = KMeans(n_clusters=n_clusters)\n",
    "kmeans_mdl.fit(umap_data)\n",
    "cluster_id = kmeans_mdl.predict(umap_data)\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "for k, center in enumerate(kmeans_mdl.cluster_centers_):\n",
    "  #plt.subplot(1, 4, 2*k+1)\n",
    "  plt.subplot(3, 4, 2*k+1)\n",
    "  plot_neighbors(umap_data, center, n = 10)\n",
    "\n",
    "  plt.subplot(3, 4, 2*k+2)\n",
    "  plt.scatter(umap_data[:,0], umap_data[:,1], c=cluster_id == k)\n",
    "plt.subplots_adjust(hspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily avg and 95% CI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_headers = om_df.columns.T\n",
    "sensorIDs_df = pd.DataFrame(columns=column_headers).transpose()\n",
    "sensorIDs_df = sensorIDs_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensorIDs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ddpm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single position as changing parameter\n",
    "\n",
    "To quickly plot single IDs for 24h averages I grabbed the Sensor_IDs from the headers of om_df\n",
    "\n",
    "Now we can use simply an index 1-444 for the profiles we have to avoid those long sensor-IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_pos = 222\n",
    "single_pos = 337\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_id = sensorIDs_df.iloc[single_pos,0]\n",
    "\n",
    "single_df = om_df[['Datetime', single_id]].copy()\n",
    "single_df['Datetime'] = pd.to_datetime(single_df['Datetime'])\n",
    "\n",
    "# temp_df[ddpm] = ddpm_df[[single_id]].copy()\n",
    "temp_df = ddpm_df[['Datetime', single_id]].copy()\n",
    "temp_df['Datetime'] = pd.to_datetime(temp_df['Datetime'])\n",
    "\n",
    "single_df = pd.merge(single_df, temp_df, on='Datetime', how='inner')\n",
    "\n",
    "temp_df = WGAN_df[['Datetime', single_id]].copy()\n",
    "temp_df['Datetime'] = pd.to_datetime(temp_df['Datetime'])\n",
    "\n",
    "single_df = pd.merge(single_df, temp_df, on='Datetime', how='inner')\n",
    "\n",
    "column_name = single_df.columns[1]\n",
    "single_df = single_df.rename(columns={column_name: 'raw'})\n",
    "\n",
    "column_name = single_df.columns[2]\n",
    "single_df = single_df.rename(columns={column_name: 'ddpm'})\n",
    "\n",
    "column_name = single_df.columns[3]\n",
    "single_df = single_df.rename(columns={column_name: 'WGAN'})\n",
    "\n",
    "\n",
    "single_df['DayOfWeek'] = single_df['Datetime'].dt.dayofweek\n",
    "\n",
    "# Create a time column in minutes past midnight\n",
    "single_df['Time'] = single_df['Datetime'].dt.hour * 60 + single_df['Datetime'].dt.minute\n",
    "\n",
    "days_of_week = sorted(single_df['DayOfWeek'].unique())\n",
    "\n",
    "day_names = [calendar.day_name[day] for day in days_of_week]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by 'DayOfWeek' and 'Time'\n",
    "groups = single_df.groupby(['DayOfWeek', 'Time'])\n",
    "\n",
    "# Func to calculate the 95% CI for 'raw', 'ddpm', and 'WGAN' & return to df\n",
    "def calculate_ci(data):\n",
    "    mean_raw = data['raw'].mean()\n",
    "    se_raw = data['raw'].sem()\n",
    "    z_score = 1.96  # Z-score for a 95% CI (assuming normal distribution)\n",
    "    margin_of_error_raw = z_score * se_raw\n",
    "    lower_bound_raw = mean_raw - margin_of_error_raw\n",
    "    upper_bound_raw = mean_raw + margin_of_error_raw\n",
    "\n",
    "    mean_ddpm = data['ddpm'].mean()\n",
    "    se_ddpm = data['ddpm'].sem()\n",
    "    margin_of_error_ddpm = z_score * se_ddpm\n",
    "    lower_bound_ddpm = mean_ddpm - margin_of_error_ddpm\n",
    "    upper_bound_ddpm = mean_ddpm + margin_of_error_ddpm\n",
    "\n",
    "    mean_wgan = data['WGAN'].mean()\n",
    "    se_wgan = data['WGAN'].sem()\n",
    "    margin_of_error_wgan = z_score * se_wgan\n",
    "    lower_bound_wgan = mean_wgan - margin_of_error_wgan\n",
    "    upper_bound_wgan = mean_wgan + margin_of_error_wgan\n",
    "\n",
    "    return pd.Series({'mean_raw': mean_raw, 'lower_bound_raw': lower_bound_raw, 'upper_bound_raw': upper_bound_raw,\n",
    "                      'mean_ddpm': mean_ddpm, 'lower_bound_ddpm': lower_bound_ddpm, 'upper_bound_ddpm': upper_bound_ddpm,\n",
    "                      'mean_wgan': mean_wgan, 'lower_bound_wgan': lower_bound_wgan, 'upper_bound_wgan': upper_bound_wgan})\n",
    "\n",
    "# Apply func for ci\n",
    "ci_results = groups.apply(calculate_ci)\n",
    "\n",
    "# Reset the index of df\n",
    "ci_results.reset_index(inplace=True)\n",
    "\n",
    "# Convert time to hour values for better plotting\n",
    "ci_results['Time'] = ci_results['Time']/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ci_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=7, figsize=(20, 10), sharey=True, sharex=True)\n",
    "fig.suptitle(f'Average power consumption per day of the week: {single_id}', fontsize=22)\n",
    "\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "# Get the actual names of the days of the week\n",
    "for i, (day, day_name) in enumerate(zip(days_of_week, day_names)):\n",
    "    day_data = ci_results[ci_results['DayOfWeek'] == day]\n",
    "    \n",
    "    ax = axes[:, i]\n",
    "    ax[0].plot(day_data['Time'], day_data['mean_raw'], label='raw', color='#487db4',  linewidth=2.5)\n",
    "    ax[0].fill_between(day_data['Time'], day_data['lower_bound_raw'], day_data['upper_bound_raw'], alpha=0.3, color='#487db4')\n",
    "    ax[0].set_title(day_name, fontsize=17, fontweight='normal')\n",
    "    #ax[0].legend(loc='upper left')\n",
    "    ax[0].set_xticks(range(0, 25, 6))\n",
    "    ax[0].set_xlim(-0.5, 24.5)\n",
    "    ax[0].set_ylim(0, 1505)\n",
    "    ax[0].set_yticks(range(0, 1505, 500))\n",
    "    \n",
    "    y_ticks_in_kW = [tick / 1000 for tick in range(0, 1505, 500)]\n",
    "    ax[0].set_yticklabels(y_ticks_in_kW, fontsize=14)\n",
    "    #ax[0].set_ylabel('mean power consumption (W)')\n",
    "    ax[0].xaxis.set_minor_locator(plt.MultipleLocator(3))\n",
    "    \n",
    "    ax[1].plot(day_data['Time'], day_data['mean_ddpm'], label='ddpm', color='teal', linewidth=2.5)\n",
    "    ax[1].plot(day_data['Time'], day_data['mean_raw'], label='raw', color='grey', linestyle='--', linewidth=3, alpha=0.99)\n",
    "    ax[1].fill_between(day_data['Time'], day_data['lower_bound_ddpm'], day_data['upper_bound_ddpm'], alpha=0.3, color='teal')\n",
    "    #ax[1].legend(loc='upper left')\n",
    "    ax[1].set_xticks(range(0, 25, 6), fontsize=14)\n",
    "    ax[1].set_xlim(-0.5, 24.5)\n",
    "    ax[1].set_yticklabels(y_ticks_in_kW,fontsize=14)\n",
    "    #ax[1].set_ylabel('mean power consumption (W)')\n",
    "    ax[1].xaxis.set_minor_locator(plt.MultipleLocator(3))\n",
    "    \n",
    "    ax[2].plot(day_data['Time'], day_data['mean_wgan'], label='WGAN', color='darkorange',  linewidth=2.5)\n",
    "    ax[2].plot(day_data['Time'], day_data['mean_raw'], label='raw', color='grey', linestyle='--', linewidth=3, alpha=0.99)\n",
    "    ax[2].fill_between(day_data['Time'], day_data['lower_bound_wgan'], day_data['upper_bound_wgan'], alpha=0.3, color='darkorange')\n",
    "    #ax[2].legend(loc='upper left')\n",
    "    ax[2].set_xticks(range(0, 25, 6))\n",
    "    ax[2].set_xlim(-0.5, 24.5)\n",
    "    ax[2].set_xlabel('hour of the day',fontsize=18)\n",
    "    ax[2].set_yticklabels(y_ticks_in_kW,fontsize=14)\n",
    "    #ax[2].set_ylabel('mean power consumption (W)')\n",
    "    ax[2].xaxis.set_minor_locator(plt.MultipleLocator(3))\n",
    "    \n",
    "        # Darken the background between 0-6 and 22-24\n",
    "    \n",
    "    night_color = 'grey'\n",
    "    salpha = 0.2\n",
    "    \n",
    "    ax[0].axvspan(0, 7, alpha=salpha, color=night_color)\n",
    "    ax[0].axvspan(21, 24, alpha=salpha, color=night_color)\n",
    "    ax[1].axvspan(0, 7, alpha=salpha, color=night_color)\n",
    "    ax[1].axvspan(21, 24, alpha=salpha, color=night_color)\n",
    "    ax[2].axvspan(0, 7, alpha=salpha, color=night_color)\n",
    "    ax[2].axvspan(21, 24, alpha=salpha, color=night_color)\n",
    "    \n",
    "    if i == 0:\n",
    "        #ax[0].set_ylabel('mean power consumption (W)',fontsize=13)\n",
    "        ax[1].set_ylabel('mean power consumption (kW)', fontsize=20)\n",
    "        #ax[2].set_ylabel('mean power consumption (W)',fontsize=13)\n",
    "\n",
    "for ax_row in axes:\n",
    "    for ax in ax_row:\n",
    "        ax.set_xticklabels(ax.get_xticks(), fontsize=14)\n",
    "\n",
    "\n",
    "# Adjust spacing between subplots and show the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(single_id+'.png', format='png')\n",
    "plt.subplots_adjust(top=0.90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#un-optimized Graph\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=7, figsize=(25, 10), sharey=True)\n",
    "fig.suptitle(f'Average power consumption per day of the week: {single_id}', fontsize=16)\n",
    "\n",
    "# Get the actual names of the days of the week\n",
    "for i, (day, day_name) in enumerate(zip(days_of_week, day_names)):\n",
    "    day_data = ci_results[ci_results['DayOfWeek'] == day]\n",
    "    \n",
    "    ax = axes[:, i]\n",
    "    ax[0].plot(day_data['Time'], day_data['mean_raw'], label='raw', color='#487db4',  linewidth=2.5)\n",
    "    ax[0].fill_between(day_data['Time'], day_data['lower_bound_raw'], day_data['upper_bound_raw'], alpha=0.3, color='#487db4')\n",
    "    ax[0].set_title(day_name)\n",
    "    ax[0].legend(loc='upper left')\n",
    "    ax[0].set_xticks(range(0, 25, 6))\n",
    "    ax[0].set_xlim(-0.5, 24.5)\n",
    "    #ax[0].set_ylabel('mean power consumption (W)')\n",
    "    ax[0].xaxis.set_minor_locator(plt.MultipleLocator(3))\n",
    "    \n",
    "    ax[1].plot(day_data['Time'], day_data['mean_ddpm'], label='ddpm', color='teal', linewidth=2.5)\n",
    "    ax[1].plot(day_data['Time'], day_data['mean_raw'], label='raw', color='grey', linestyle='--', linewidth=3, alpha=0.99)\n",
    "    ax[1].fill_between(day_data['Time'], day_data['lower_bound_ddpm'], day_data['upper_bound_ddpm'], alpha=0.3, color='teal')\n",
    "    ax[1].legend(loc='upper left')\n",
    "    ax[1].set_xticks(range(0, 25, 6))\n",
    "    ax[1].set_xlim(-0.5, 24.5)\n",
    "    #ax[1].set_ylabel('mean power consumption (W)')\n",
    "    ax[1].xaxis.set_minor_locator(plt.MultipleLocator(3))\n",
    "    \n",
    "    ax[2].plot(day_data['Time'], day_data['mean_wgan'], label='WGAN', color='orange',  linewidth=2.5)\n",
    "    ax[2].plot(day_data['Time'], day_data['mean_raw'], label='raw', color='grey', linestyle='--', linewidth=3, alpha=0.99)\n",
    "    ax[2].fill_between(day_data['Time'], day_data['lower_bound_wgan'], day_data['upper_bound_wgan'], alpha=0.3, color='orange')\n",
    "    ax[2].legend(loc='upper left')\n",
    "    ax[2].set_xticks(range(0, 25, 6))\n",
    "    ax[2].set_xlim(-0.5, 24.5)\n",
    "    ax[2].set_xlabel('hour of the day',fontsize=13)\n",
    "    #ax[2].set_ylabel('mean power consumption (W)')\n",
    "    ax[2].xaxis.set_minor_locator(plt.MultipleLocator(3))\n",
    "    \n",
    "        # Darken the background between 0-6 and 22-24\n",
    "    ax[0].axvspan(0, 7, alpha=0.2, color='grey')\n",
    "    ax[0].axvspan(21, 24, alpha=0.2, color='grey')\n",
    "    ax[1].axvspan(0, 7, alpha=0.2, color='grey')\n",
    "    ax[1].axvspan(21, 24, alpha=0.2, color='grey')\n",
    "    ax[2].axvspan(0, 7, alpha=0.2, color='grey')\n",
    "    ax[2].axvspan(21, 24, alpha=0.2, color='grey')\n",
    "    \n",
    "    if i == 0:\n",
    "        ax[0].set_ylabel('mean power consumption (W)',fontsize=13)\n",
    "        ax[1].set_ylabel('mean power consumption (W)', fontsize=13)\n",
    "        ax[2].set_ylabel('mean power consumption (W)',fontsize=13)\n",
    "  \n",
    "# Adjust spacing between subplots and show the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
